# -*- coding: utf-8 -*-
"""Project - Hand Written Digit Images using GAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18AenGpSWr4HczpCdnIr60DCPc3NovdnP

## Generate Handwritten digit images form MNIST Dataset using GAN

Generative Adversarial Model is a form of unsupervised machine learning  algorithm. It has two models - Generator and Discriminator, where a generator tries to generate new data and the discriminator tries to classify the data as either real or fake. Based on the discriminators loss, generator tries to adjust weights on its layers and creates new data to increase the dicriinator loss.

# Import the necessary libraries and load the data

We import the necessary libraries:
* **numpy** - the numerical computing library to provide mathematical operations on arrays \
* **keras** - high level neural network API for training the deep learning models \
* **tensorflow** - open-source library for mchine learning and deep learning \


**MNIST** is a well known dataset for handwritten digitd which we use to train our model to generate new handwritten digit images.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import keras
import keras.backend as K
from keras.layers import Input, Dense, Activation, LeakyReLU, BatchNormalization
from keras.models import Sequential
from keras.optimizers import Adam
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
# %matplotlib inline

import tensorflow as tf
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()

"""# GAN design
GANs are a structured probabilistic model which consists of 2 adversarial networks - a generator network G and a discriminator network D. \
**Generative model (G)** – It is a de-convolutional neural network, which captures the data distribution and represents a probability distribution pmodel(x). The generator takes in observed variables x from the given dataset and noise vectors z(latent variables) sampled from pz and generates images. The generated synthetic images are blended with the true images from a dataset and fed into the discriminator.   \
For the generator, it has to generate a 28*28 pixels grayscale fake images from random noise. It will accept a 1-D array and output an image. This can be achieved using a simple feedforward neural network of hidden layers, which reshapes the 1-D array to a 2-D array by up-sampling it using convolutional LeakyReLU layers, batch normalization, and activation functions. \
Input layer: The input layer of the generator takes a random noise vector as input which is drawn from a simple distribution. The size of the noise vector depends on size of the latent space, we have used 100-dimensional.\
We add layers on top of it to increase the spatial resolution to (28, 28) to match the size of the MNIST images. We add one or more convolutional layers to refine the features and generate the final image. To stabilize the training process, batch normalization layers are added after each convolutional layer. These layers help to normalize the activations and reduce the covariate shift, making the training process more stable. Finally, activation functions, leaky ReLU has be added after each convolutional layer to introduce non-linearity into the model. \
The final layer of the generator is the output layer, which produces the generated image. The activations from the final layer can be passed through a sigmoid activation function to produce values between 0 and 1, which can be interpreted as pixel values. \


**Discriminative model (D)** – It is a convolutional neural network that estimates the probability that a sample came from the training data(real) rather than generative model G(fake). \
It has been created as a simple fully connected neural network with one hidden layer with the leaky ReLU activation. It takes the images as input and we down-sample it using convolutional LeakyReLU layers, dense layers, and activation functions. The activation function we use is a sigmoid as we want to output the probability of the input being a real opr a fake image. \
I have used the Adam optimizer for the discriminator optimiser. \

The gan is basically a generaotor and a discriminator layers added sequentially.





"""

def make_simple_GAN(sample_size,
                    g_hidden_size,
                    d_hidden_size,
                    leaky_alpha,
                    g_learning_rate,
                    d_learning_rate):
    K.clear_session()

    img_rows = 28
    img_cols = 28
    channels = 1
    img_shape = (img_rows, img_cols, channels)
    latent_dim = 100

    generator = Sequential([
        Dense(g_hidden_size, input_shape=(sample_size,)),
        LeakyReLU(alpha=leaky_alpha),
        BatchNormalization(momentum=0.8),
        Dense(512),
        LeakyReLU(alpha=leaky_alpha),
        BatchNormalization(momentum=0.8),
        Dense(1024),
        LeakyReLU(alpha=leaky_alpha),
        BatchNormalization(momentum=0.8),
        Dense(np.prod(img_shape)),
        Activation('tanh')
    ], name='generator')

    discriminator = Sequential([
        Dense(d_hidden_size, input_shape=(784,)),
        LeakyReLU(alpha=leaky_alpha),
        Dense(256),
        LeakyReLU(alpha=leaky_alpha),
        Dense(1),
        Activation('sigmoid')
    ], name='discriminator')

    gan = Sequential([
        generator,
        discriminator
    ])

    discriminator.compile(optimizer=Adam(lr=d_learning_rate), loss='binary_crossentropy')
    gan.compile(optimizer=Adam(lr=g_learning_rate), loss='binary_crossentropy')

    return gan, generator, discriminator

"""# Make the discriminator trainable

We need to have an option to make the discriminator layers trainable and non-trainable when needed. When we train the discriminator, we want the discriminator to change its weights thus want it to be trainable. \
When we train the GAN, the back-propogation should allow the generator to change its weight. We do not want the discriminator to not change it's weight during this time and thus make it non-trainable.
"""

def make_trainable(model, trainable):
    for layer in model.layers:
        layer.trainable = trainable

"""# Noise/Latent  variables

Typically, noise is drawn from a Gaussian or uniform distribution because many complicated systems can be modelled successfully as normally distributed noise. We have used normal distribution as it encodes the maximum amount of uncertainty over the real numbers thus inserting the least amount of prior knowledge into a model. \
The latent space in a GAN is a continuous space that can be sampled to generate new data by learning the underlying probability distribution of the training data.

"""

def make_latent_samples(n_samples, sample_size):
    return np.random.normal(loc=0, scale=1, size=(n_samples, sample_size))

"""# Preprocessing and Deprocessing of data

The MNIST dataset contains pixel values which are stored as 8-bit integers, with values ranging from 0 to 255. This can cause issues during training, as neural networks typically work best with data that is centered around zero and has a small variance. When the pixel values are not scaled, the network may struggle to learn the underlying patterns in the data, as the large range of values can cause the gradients to become unstable. \
To address this, we rescale the MNIST dataset between -1 and 1. This rescaling step helps to standardize the data, making it easier for the network to learn the underlying patterns in the data. Additionally, training with rescaled data can be faster and more stable, as the network can converge more quickly and with fewer issues related to unstable gradients. \
Similarly, deprocessing is the opposite process of preprocess to be able to view the images.

"""

def preprocess(x):
    x = x.reshape(-1, 784) # 784=28*28
    x = np.float64(x)
    x = (x / 255 - 0.5) * 2
    x = np.clip(x, -1, 1)
    return x

def deprocess(x):
    x = (x / 2 + 1) * 255
    x = np.clip(x, 0, 255)
    x = np.uint8(x)
    x = x.reshape(28, 28)
    return x

"""# Labels for real and fake data
We make label = 1 for the real data and label = 0 for the fake data and append it to our dataset. This is used to identify how many images were identified correctly as real or fake by the discriinator, thus calculating the generator and discriminator loss.

"""

def make_labels(size):
    return np.ones([size, 1]), np.zeros([size, 1])

X_train_real = preprocess(X_train)
X_test_real  = preprocess(X_test)

y_real_10, y_fake_10 = make_labels(10)

"""# Train the model

Next, we setup all the hyperparaeters for our model including the number of epochs, the batch size and different learning rates. \
We create the GAN, generator and discriminator for using these hyperparameters and run the model for the number of epochs. \
We train the discriminator and then train the generator via GAN.

"""

from keras.layers import Flatten

# hyperparameters
sample_size     = 100     # latent sample size (100 random numbers)
g_hidden_size   = 128
d_hidden_size   = 128
leaky_alpha     = 0.05    # for leaky ReLU
g_learning_rate = 0.0001  # learning rate for the generator
d_learning_rate = 0.001   # learning rate for the discriminator
epochs          = 1000     # number of times to run the model
batch_size      = 64      # training batch size
eval_size       = 16      # evaluate size
smooth          = 0.1

# labels for the batch size and the test size
y_train_real, y_train_fake = make_labels(batch_size)
y_eval_real,  y_eval_fake  = make_labels(eval_size)

# create GAN, generator and discriminator
gan, generator, discriminator = make_simple_GAN(
    sample_size,
    g_hidden_size,
    d_hidden_size,
    leaky_alpha,
    g_learning_rate,
    d_learning_rate)

losses = []
for e in range(epochs):
    for i in range(len(X_train_real)//batch_size):
        # real MNIST digit images
        X_batch_real = X_train_real[i*batch_size:(i+1)*batch_size]

        # latent samples and the generated digit images
        latent_samples = make_latent_samples(batch_size, sample_size)
        X_batch_fake = generator.predict_on_batch(latent_samples)

        # train the discriminator to detect real and fake images
        make_trainable(discriminator, True)
        discriminator.train_on_batch(X_batch_real, y_train_real * (1 - smooth))
        discriminator.train_on_batch(X_batch_fake, y_train_fake)

        # train the generator via GAN
        make_trainable(discriminator, False)
        gan.train_on_batch(latent_samples, y_train_real)

    # evaluate
    X_eval_real = X_test_real[np.random.choice(len(X_test_real), eval_size, replace=False)]

    latent_samples = make_latent_samples(eval_size, sample_size)
    X_eval_fake = generator.predict_on_batch(latent_samples)

    d_loss  = discriminator.test_on_batch(X_eval_real, y_eval_real)
    d_loss += discriminator.test_on_batch(X_eval_fake, y_eval_fake)
    g_loss  = gan.test_on_batch(latent_samples, y_eval_real)

    losses.append((d_loss, g_loss))

    print("Epoch: {:>3}/{} Discriminator Loss: {:>6.4f} Generator Loss: {:>6.4f}".format(
        e+1, epochs, d_loss, g_loss))

"""#Plot the generator and discriminator loss

The generator tries to increase the discriminator loss and the discriminator tries to increase the generator loss. \
As the generator loss decreases, the discriminator loss should increase as we see in the graph. It means that the images are more realistic and the discriminator is not being able to distinguish real images from fake images as easiliy as it could do earlier. \
We want both the generator loss and the discriminator loss to be as low as possible. \
We can check these values by playing with the hyperparameters, like increasing the number of epochs or having different values of learning rates. We can also try to add more layers to the network to see if it improves our losses and the images.  
"""

losses = np.array(losses)

fig, ax = plt.subplots()
plt.plot(losses.T[0], label='Discriminator')
plt.plot(losses.T[1], label='Generator')
plt.title("Training Losses")
plt.legend()
plt.show()

"""# Plot the fake images generated by the generator
 Here we try to see what kind of hand written digit images our generator is able to plot.
"""

latent_samples = make_latent_samples(20, sample_size)
generated_digits = generator.predict(latent_samples)

plt.figure(figsize=(10, 8))
for i in range(20):
    img = deprocess(generated_digits[i])
    plt.subplot(4, 5, i+1)
    plt.imshow(img, cmap='gray')
    plt.xticks([])
    plt.yticks([])
plt.tight_layout()
plt.show()